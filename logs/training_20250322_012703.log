Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|#####     | 1/2 [00:00<00:00,  1.34it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.69it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.63it/s]
/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py:390: UserWarning: The requested device cuda:0 is also being used for training. For higher throughput and to avoid out-of-memory errors, it is recommended to use a dedicated device for vLLM. If this is intentional, you may ignore this warning but should adjust `vllm_gpu_memory_utilization` accordingly.
  warnings.warn(
Traceback (most recent call last):
  File "/workspace/rl_manipulation_human_baseline/train.py", line 171, in <module>
    pre_trainer = GRPOTrainer(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 404, in __init__
    self.llm = LLM(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1022, in inner
    return fn(*args, **kwargs)
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 486, in from_engine_args
    engine_config = engine_args.create_engine_config(usage_context)
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1127, in create_engine_config
    model_config = self.create_model_config()
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 1047, in create_model_config
    return ModelConfig(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/config.py", line 366, in __init__
    self.multimodal_config = self._init_multimodal_config(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/config.py", line 427, in _init_multimodal_config
    if ModelRegistry.is_multimodal_model(architectures):
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 460, in is_multimodal_model
    model_cls, _ = self.inspect_model_cls(architectures)
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 416, in inspect_model_cls
    model_info = self._try_inspect_model_cls(arch)
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 391, in _try_inspect_model_cls
    return _try_inspect_model_cls(model_arch, self.models[model_arch])
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 319, in _try_inspect_model_cls
    return model.inspect_model_cls()
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 290, in inspect_model_cls
    return _run_in_subprocess(
  File "/workspace/rl_manipulation_human_baseline/.venv/lib/python3.10/site-packages/vllm/model_executor/models/registry.py", line 522, in _run_in_subprocess
    returned = subprocess.run(_SUBPROCESS_COMMAND,
  File "/usr/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/usr/lib/python3.10/subprocess.py", line 1154, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
  File "/usr/lib/python3.10/subprocess.py", line 2021, in _communicate
    ready = selector.select(timeout)
  File "/usr/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
